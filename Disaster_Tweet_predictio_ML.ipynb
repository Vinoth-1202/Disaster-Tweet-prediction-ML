{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:10:43.473566Z","iopub.execute_input":"2023-07-31T05:10:43.473961Z","iopub.status.idle":"2023-07-31T05:10:43.529562Z","shell.execute_reply.started":"2023-07-31T05:10:43.473933Z","shell.execute_reply":"2023-07-31T05:10:43.528703Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/train.csv\n/kaggle/input/nlp-getting-started/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **NLP DISASTER TWEET**","metadata":{"id":"um0w88FNapbn"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport nltk\nimport re\nimport os\nimport nltk\n\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.metrics import accuracy_score\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nnltk.download(\"stopwords\")\nnltk.download(\"punkt\")","metadata":{"id":"Ok-jtjV-aO-l","outputId":"74badec3-75df-4577-d4b4-095e2d44d2a4","execution":{"iopub.status.busy":"2023-07-31T05:10:43.531277Z","iopub.execute_input":"2023-07-31T05:10:43.532260Z","iopub.status.idle":"2023-07-31T05:10:43.544021Z","shell.execute_reply.started":"2023-07-31T05:10:43.532218Z","shell.execute_reply":"2023-07-31T05:10:43.542936Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"},{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# **LOAD DATA**","metadata":{"id":"Z4FTvEQra0eR"}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\ntest=pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"id":"fwzEis6X22I4","execution":{"iopub.status.busy":"2023-07-31T05:10:43.545487Z","iopub.execute_input":"2023-07-31T05:10:43.546350Z","iopub.status.idle":"2023-07-31T05:10:43.626852Z","shell.execute_reply.started":"2023-07-31T05:10:43.546308Z","shell.execute_reply":"2023-07-31T05:10:43.625556Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Number of rows and columns\nprint('head',df.head())\n\n# Data types of each column\ndata_types = df.dtypes\n\n# Summary statistics\nsummary_stats = df.describe()\n\n# Missing values\nmissing_values = df.isnull().sum()\n\n\nprint(\"rows,cols\",num_rows,num_cols)\nprint('types=',data_types)\nprint('stats=',summary_stats)\nprint('missing values=',missing_values)","metadata":{"execution":{"iopub.status.busy":"2023-07-31T05:13:25.323250Z","iopub.execute_input":"2023-07-31T05:13:25.323669Z","iopub.status.idle":"2023-07-31T05:13:25.346984Z","shell.execute_reply.started":"2023-07-31T05:13:25.323637Z","shell.execute_reply":"2023-07-31T05:13:25.345932Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"head    id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  \nrows,cols 7613 5\ntypes= id           int64\nkeyword     object\nlocation    object\ntext        object\ntarget       int64\ndtype: object\nstats=                  id      target\ncount   7613.000000  7613.00000\nmean    5441.934848     0.42966\nstd     3137.116090     0.49506\nmin        1.000000     0.00000\n25%     2734.000000     0.00000\n50%     5408.000000     0.00000\n75%     8146.000000     1.00000\nmax    10873.000000     1.00000\nmissing values= id             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Data Preprocessing**","metadata":{"id":"s5-ogIgCpzXQ"}},{"cell_type":"code","source":"# Remove URLs and special characters using a for loop\ncleaned_text_list = []\nfor text in df['text']:\n    text=text.lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n    # Remove \"@\" symbol\n    text = text.replace('@', '')\n\n    # Remove special characters (keep only alphanumeric characters and whitespace)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n\n    # Tokenize the text\n    tokens = word_tokenize(text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n    # Uncomment either the stemming or lemmatization section based on your choice\n    # Stemming\n    stemmer = PorterStemmer()\n    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n    cleaned_text_list.append(' '.join(stemmed_tokens))\n    \n# Add the cleaned_text_list as a new column in the DataFrame\ndf['cleaned_text'] = cleaned_text_list\n# data split\nx=df['cleaned_text']\ny=df['target']\nx","metadata":{"id":"7aVM4N6rp9hn","outputId":"85ea29f5-d7f9-4d1f-d31d-6ec8e9f250dc","execution":{"iopub.status.busy":"2023-07-31T05:10:43.640533Z","iopub.status.idle":"2023-07-31T05:10:43.641709Z","shell.execute_reply.started":"2023-07-31T05:10:43.641366Z","shell.execute_reply":"2023-07-31T05:10:43.641399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Feature Extraction**","metadata":{"id":"rupZk8Nho9zi"}},{"cell_type":"code","source":"tfidf_vectorizer = TfidfVectorizer()\nX= tfidf_vectorizer.fit_transform(df['cleaned_text'])\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42)","metadata":{"id":"EXqFjGX5UvBu","execution":{"iopub.status.busy":"2023-07-31T05:10:43.642981Z","iopub.status.idle":"2023-07-31T05:10:43.643682Z","shell.execute_reply.started":"2023-07-31T05:10:43.643414Z","shell.execute_reply":"2023-07-31T05:10:43.643441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Classification**\n","metadata":{"id":"X1ThKHrcHkCT"}},{"cell_type":"code","source":"cl_svc=SVC(kernel='linear')\ncl_svc.fit(X_train, y_train)\n\ncl_lr=LogisticRegression()\ncl_lr.fit(X_train,y_train)\n\ncl_knn = KNeighborsClassifier(n_neighbors=2)\ncl_knn.fit(X_train, y_train)\n\ncl_svc_y=cl_svc.predict(X_test)\ncl_lr_y=cl_lr.predict(X_test)\ncl_knn_y=cl_knn.predict(X_test)\n\nsvc_accuracy=accuracy_score(cl_svc_y,y_test)\nlr_accuracy=accuracy_score(cl_lr_y,y_test)\nknn_accuracy=accuracy_score(cl_knn_y,y_test)\n\nprint('svm:',classification_report (y_test,cl_svc_y))\nprint('logistic_Regression:',classification_report (y_test,cl_lr_y))\nprint('knn',classification_report (y_test,cl_knn_y))\n\ntrain_svm_accuracy=cl_svc.score(X_train, y_train)\ntrain_lr_accuracy=cl_lr.score(X_train, y_train)\ntrain_knn_accuracy=cl_knn.score(X_train, y_train)\n\nprint(\"svc_Accuracy:\",svc_accuracy )\n#print(\"train_svm_Accuracy:\", train_svm_accuracy)\nprint(\"logistic_Regression_Accuracy:\",lr_accuracy )\n#print(\"train_lr_Accuracy:\", train_lr_accuracy)\nprint(\"knn_Accuracy:\",knn_accuracy )\n#print(\"train_knn_Accuracy:\", train_knn_accuracy)","metadata":{"id":"D3E3pNx3Hs1v","outputId":"c7f9fd63-8a47-4f07-9423-9e198da5b12d","execution":{"iopub.status.busy":"2023-07-31T05:39:43.922863Z","iopub.execute_input":"2023-07-31T05:39:43.923279Z","iopub.status.idle":"2023-07-31T05:39:54.252352Z","shell.execute_reply.started":"2023-07-31T05:39:43.923246Z","shell.execute_reply":"2023-07-31T05:39:54.251105Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"svm:               precision    recall  f1-score   support\n\n           0       0.78      0.86      0.82       874\n           1       0.78      0.68      0.73       649\n\n    accuracy                           0.78      1523\n   macro avg       0.78      0.77      0.77      1523\nweighted avg       0.78      0.78      0.78      1523\n\nlogistic_Regression:               precision    recall  f1-score   support\n\n           0       0.77      0.90      0.83       874\n           1       0.82      0.65      0.72       649\n\n    accuracy                           0.79      1523\n   macro avg       0.80      0.77      0.78      1523\nweighted avg       0.80      0.79      0.79      1523\n\nknn               precision    recall  f1-score   support\n\n           0       0.72      0.91      0.80       874\n           1       0.81      0.53      0.64       649\n\n    accuracy                           0.75      1523\n   macro avg       0.77      0.72      0.72      1523\nweighted avg       0.76      0.75      0.73      1523\n\nsvc_Accuracy: 0.7839789888378201\nlogistic_Regression_Accuracy: 0.7905449770190414\nknn_Accuracy: 0.7465528562048588\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Test Data Preprocessing**\n","metadata":{"id":"XMjGwrzlmlkL"}},{"cell_type":"code","source":"# Remove URLs and special characters using a for loop\ncleaned_test_list = []\nfor text in test['text']:\n    text=text.lower()\n    # Remove URLs\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n\n    # Remove \"@\" symbol\n    text = text.replace('@', '')\n\n    # Remove special characters (keep only alphanumeric characters and whitespace)\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n\n    # Tokenize the text\n    tokens = word_tokenize(text)\n\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n\n    # Stemming\n    stemmer = PorterStemmer()\n    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n    cleaned_test_list.append(' '.join(stemmed_tokens))\n    \n# Add the cleaned_test_list as a new column in the DataFrame\ntest['cleaned_text'] = cleaned_test_list\ntest_x = tfidf_vectorizer.transform(test['cleaned_text'])","metadata":{"id":"ld39ki4xnPe2","execution":{"iopub.status.busy":"2023-07-31T05:10:54.328976Z","iopub.execute_input":"2023-07-31T05:10:54.329456Z","iopub.status.idle":"2023-07-31T05:10:56.379442Z","shell.execute_reply.started":"2023-07-31T05:10:54.329417Z","shell.execute_reply":"2023-07-31T05:10:56.378302Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"# **Predicting**","metadata":{"id":"gwQtBmJXvLqu"}},{"cell_type":"code","source":"test_pred_svc=cl_svc.predict(test_x)\ntest_pred_lr=cl_lr.predict(test_x)\ntest_pred_knn=cl_knn.predict(test_x)","metadata":{"id":"lkBNoJz7oJ_9","outputId":"125580d7-9386-44ac-c37b-88ab27ab6716","execution":{"iopub.status.busy":"2023-07-31T05:10:56.380987Z","iopub.execute_input":"2023-07-31T05:10:56.381442Z","iopub.status.idle":"2023-07-31T05:10:58.853063Z","shell.execute_reply.started":"2023-07-31T05:10:56.381403Z","shell.execute_reply":"2023-07-31T05:10:58.851697Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"# **Result**","metadata":{}},{"cell_type":"code","source":"submissions = pd.read_csv(r\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\nsubmissions['target']=test_pred_svc\nsubmissions.to_csv(\"submissions.csv\", index=False)","metadata":{"id":"1gozBAX4szIq","outputId":"92e61e8a-d291-456d-be5f-d6dae0d5c000","execution":{"iopub.status.busy":"2023-07-31T05:10:58.855784Z","iopub.execute_input":"2023-07-31T05:10:58.856283Z","iopub.status.idle":"2023-07-31T05:10:58.872856Z","shell.execute_reply.started":"2023-07-31T05:10:58.856241Z","shell.execute_reply":"2023-07-31T05:10:58.871454Z"},"trusted":true},"execution_count":42,"outputs":[]}]}